#This script contains the functions for obtaining flight details, tracking, geo-referencing

#load packages here in case they dont work when loaded from inside the function below
import numpy as np
from filterpy.kalman import KalmanFilter
from filterpy.stats import mahalanobis

#This function accepts filename and directories for the video file (where the .srt file is stored) and the drone log files to obtain the correct flight log for the video file. It returns flight details at the default resolution (logged once every 200 ms) from both the flight log and the srt file. Note that this function works on srt files generated by the DJI mini 3 pro; in case the formart of the data in the srt file and the log file are different, the function will need to be modified accordingly. Flight log csvs are obtained using the phantomhelp website - https://www.phantomhelp.com/logviewer/upload/

def get_flight_details(filename, pth, pth_log, filename_log):
    
    import pandas as pd
    from datetime import datetime, timedelta
    import os
    import pysrt
    import re
    
    #Read the srt file
    srt = pysrt.open(pth+filename+".srt")
    
    #To get the video start time (for DJI mini models)
    sub_start_line = [subtitle.text for subtitle in srt][1]
    video_start_time = re.findall(r'\n(.*?)\n', sub_start_line, re.DOTALL)[0]
    srt_dt = datetime.strptime(video_start_time, '%Y-%m-%d %H:%M:%S.%f')#.replace(microsecond=0)


    #To get video time data from the subtitles
    s_text = [subtitle.text for subtitle in srt]
    video_time = []
    for s in s_text:
        video_time.append(re.findall(r'\n(.*?)\n', s, re.DOTALL)[0].split()[1])
        
    #Store time data into a data frame
    df_video = pd.DataFrame({"time": video_time})
    df_video['time'] = df_video['time'].apply(lambda dt: datetime.strptime(dt, '%H:%M:%S.%f')).dt.time
    df_video = df_video.groupby('time').mean().reset_index()

    
    #Obtain flight start time from the log file name (for DJI mini drones; flight start time may beed to be obtained form different sources for other drones)
    flight_str_time = re.findall(r'\[(.*?)\]', filename_log)[0]
    flight_str_time = datetime.strptime(str(video_start_time[0:10]+" "+flight_str_time), '%Y-%m-%d %H-%M-%S')
    
    #Read and store all relevant flight data into a data frame
    df_flight = pd.read_csv(pth_log+filename_log, header=1)
    
    #Note that these column names may vary according to the drone model; several columns, like home longitude are only for reference and are not used in geo-referencing.
    df_flight = df_flight[['CUSTOM.date [local]', 'CUSTOM.updateTime [local]', "OSD.flyTime [s]","OSD.latitude","OSD.longitude","OSD.height [ft]","OSD.altitude [ft]","OSD.hSpeed [MPH]","OSD.pitch","OSD.roll","OSD.yaw [360]","OSD.directionOfTravel","GIMBAL.pitch","GIMBAL.yaw [360]","HOME.latitude","HOME.longitude","HOME.distance [ft]"]]
    
    #Obtain flight start time from inside the log file; this may not always be accurate
    flight_str_time_log = datetime.combine(datetime.strptime(str(df_flight['CUSTOM.date [local]'][0]), '%m/%d/%Y').date(),datetime.strptime(str(df_flight['CUSTOM.updateTime [local]'][0]), '%I:%M:%S.%f %p').time()).replace(microsecond=0)
    
    #Corrected timestamps of flight time are obtained by incorporating 'true' flight start time
    df_flight["time"] = (df_flight['CUSTOM.updateTime [local]'].apply(lambda x: datetime.strptime(x, '%I:%M:%S.%f %p')) - (flight_str_time_log-flight_str_time)).apply(lambda dt: dt.time())
    
    return df_video, df_flight, srt_dt


#This function accepts flight data and pixel positions of objects and converts them to geo-coordinates. Since flight parameters can lag by a few seconds, we use an offset value (time_off) to accurately synchronise all files. See section 2.1 of our manuscript on how to check for the offset value. Height offset is to be used in cases when the drone is not deployed from the same height as where the animals are to be tracked
def pix_to_pos(data, df_flight, srt_dt, time_off, height_ft_off, log_freq, h_pix, v_pix, h_fov, v_fov):
   
    import pandas as pd
    import numpy as np
    from datetime import datetime
    
    # Function to find the closest match of each row in one dataframe to another row of the 2nd dataframe i.e., match rows with the lowest time difference; this is reuired since timestamps in the two dataframes may not always match exactly
    def find_closest(df1, df2, offset1):
        df1['datetime_time'] = df1['time'].apply(lambda x: datetime.combine(datetime.min, x))
        df2['datetime_time'] = df2['time'].apply(lambda x: datetime.combine(datetime.min, x))
        df1 = df1.reset_index(drop=True)
        df2 = df2.reset_index(drop=True)
    
        # Calculate the difference and find the index of the minimum difference
        min_index = []
        for c in range(0,len(df1)):
            diff = (df1['datetime_time'][c]-df2['datetime_time']).abs()
            if (diff.idxmin()+offset1)<len(df2):
                min_index.append(diff.idxmin()+offset1)
            else:
               min_index.append(np.nan)
        # Create a new dataframe with the same columns as df2
        result = pd.DataFrame(columns=df2.columns)
            
        # Populate the result dataframe
        for idx in min_index:
            if pd.isna(idx):
                empty_row = pd.Series([np.nan] * len(df2.columns), index=df2.columns)
                result = pd.concat([result, empty_row.to_frame().T], ignore_index=True)
            else:
                result = pd.concat([result, df2.loc[[idx]]], ignore_index=True)
        return result
    
    #convert flight time to real time
    data["time"] = pd.to_timedelta(data["video_time"], unit='s') + srt_dt 
    data["time"] = data["time"].dt.time

    #match timestamps from the video (srt) and flight log file with an estimated offset; note that time_off is defined as the estimated offset time (in seconds) * logging frequency in the flight log. E.g. if estimated lag is 2s and logging frequency is 200ms, off = 2*5 = 10
    closest_rows = find_closest(data, df_flight, int(time_off*log_freq))
    df_final = pd.concat([data.reset_index(), closest_rows.drop(['datetime_time','time'],axis=1).reset_index()], axis=1).drop(['datetime_time'],axis=1)
    
    #df_final['OSD.height [ft]'] = df_final["OSD.altitude [ft]"]# This is only useful for sea level areas and doesnt require a height offset to impement geo-referencing
    df_final['OSD.height [ft]'] = df_final['OSD.height [ft]'] + height_ft_off
    
    #implement the geo-referencing algorithm, as defined in the manuscript
    df_final['y_image_deg'] = (df_final['GIMBAL.pitch']*-1) - ((v_pix/2 - df_final['y_image'])/v_pix * v_fov)
    df_final['dist_obj_v'] = df_final['OSD.height [ft]'] / np.tan(np.radians(df_final['y_image_deg']))
    df_final['dist_obj_g'] = df_final['OSD.height [ft]'] / np.sin(np.radians(df_final['y_image_deg']))
    
    df_final['deg_h'] = (df_final['x_image'] - h_pix/2) / h_pix * h_fov
        
    df_final['dist_h'] = df_final['dist_obj_g'] / np.cos(np.radians(df_final['deg_h']))
    df_final['dist_h'] = (df_final['dist_h']**2 - df_final['OSD.height [ft]']**2)**0.5
    df_final['dist_drone_obj'] = df_final['dist_h']
    
    df_final['dist_obj_v'] = pd.to_numeric(df_final['dist_obj_v'])
    df_final['dist_drone_obj'] = pd.to_numeric(df_final['dist_drone_obj'])
    df_final['deg_gimb'] = np.sign(df_final['deg_h'])*np.abs(np.degrees(np.arccos(round(df_final['dist_obj_v'] / df_final['dist_drone_obj'],8))))
    
    df_final['obj_yaw'] = df_final['OSD.yaw [360]'] + df_final['deg_gimb']
    df_final['obj_lat'] = (np.cos(np.radians(df_final['obj_yaw'])) * df_final['dist_drone_obj'] / (111110*3.32808)) + df_final['OSD.latitude']
    df_final['obj_lon'] = (np.sin(np.radians(df_final['obj_yaw'])) * df_final['dist_drone_obj'])/(111110*np.cos(np.radians(np.round(df_final['OSD.latitude'])))*3.32808) + df_final['OSD.longitude']
    
    # Checking if drone is moving at each instance. This works if you are only converting 1 data point per second
    df_check = df_final[['video_time','GIMBAL.pitch','OSD.yaw [360]']].drop_duplicates(subset='video_time', keep="first")
    
    df_check['gimbal_move'] = ((df_check['GIMBAL.pitch'] - df_check['GIMBAL.pitch'].shift(1)).abs() > 0.5) | ( (df_check['GIMBAL.pitch'] - df_check['GIMBAL.pitch'].shift(-1)).abs() > 0.5)#if gimbal moves by at least 0.5 degrees within 1 sec
    df_check['yaw_move'] = ((df_check['OSD.yaw [360]'] - df_check['OSD.yaw [360]'].shift(1)).abs() > 2) | ( (df_check['OSD.yaw [360]'] - df_check['OSD.yaw [360]'].shift(-1)).abs() > 2)#if drone rotates by at least 2 degrees within 1 sec
    
    df_final = pd.merge(df_final, df_check[['video_time', 'gimbal_move', 'yaw_move']], on='video_time', how='left')

    df_final['drone_moving'] = ((df_final['OSD.hSpeed [MPH]'] > 3) | (df_final['gimbal_move'] == True) | (df_final['yaw_move'] == True)).astype(int)#if the drone is moving above ~5kmph

    df_final = df_final.rename(columns={
    df_final.columns[0]: 'pix_coord_tl_x',
    df_final.columns[1]: 'pix_coord_tl_y',
    df_final.columns[2]: 'pix_coord_bl_x',
    df_final.columns[3]: 'pix_coord_bl_y',
    df_final.columns[4]: 'pix_coord_br_x',
    df_final.columns[5]: 'pix_coord_br_y',
    df_final.columns[6]: 'pix_coord_tr_x',
    df_final.columns[7]: 'pix_coord_tr_y',
    df_final.columns[8]: 'ID_prob',
    df_final.columns[9]: 'ID',
    df_final.columns[10]: 'Tracking_YN',
    df_final.columns[11]: 'time_since_update',
    df_final.columns[12]: 'frame_no'
    })

    
    return df_final

#This function is used to initiate and update object trackers to obtain their tracks. It has been adapted and modified from Rathore et al., 2023.
class yolo:

        import numpy as np
        from filterpy.kalman import KalmanFilter
        from filterpy.stats import mahalanobis
        
        def overlap_area(box1, box2):
            
            from shapely.geometry import Polygon
            poly1 = Polygon(box1)
            poly2 = Polygon(box2)
            
            # Check if the polygons overlap
            if poly1.intersects(poly2):
                intersection = poly1.intersection(poly2)
                return intersection.area
            else:
                return 0

        def union_area(box1, box2):
            
            from shapely.geometry import Polygon    
            poly1 = Polygon(box1)
            poly2 = Polygon(box2)
            
            return poly1.union(poly2).area

        def bbox_iou(box1, box2):#8 coordinates for four corners, anticlockwise from top-left

            corners1 = [(box1[i], box1[i+1]) for i in range(0, len(box1), 2)]
            corners2 = [(box2[i], box2[i+1]) for i in range(0, len(box2), 2)]

            # Convert the list into a tuple of 4 corners
            corners_tuple1 = tuple(corners1)
            corners_tuple2 = tuple(corners2)
            
            overlap = yolo.overlap_area(corners_tuple1, corners_tuple2)
            union = yolo.union_area(corners_tuple1, corners_tuple2)

            return float(overlap) / union
        
        def do_nms(new_boxes, nms_thresh):
            # do nms
            sorted_indices = np.argsort(-new_boxes[:,8])
            boxes=new_boxes.tolist()

            for i in range(len(sorted_indices)):

                index_i = sorted_indices[i]

                if new_boxes[index_i,8] == 0: continue

                for j in range(i+1, len(sorted_indices)):
                    index_j = sorted_indices[j]
                    # anything with certainty above 1 is untouchable
                    if boxes[index_j][8]>1:
                        continue
                    if yolo.bbox_iou(boxes[index_i][0:8], boxes[index_j][0:8]) > nms_thresh:
                        new_boxes[index_j,8] = 0

            return new_boxes

        
        def convert_bbox_to_kfx(bbox):
            """
            Takes a bounding box in the form [x1,y1,x2,y2,x3,y3,x4,y4] (coordinates are anticlockwise top left) and returns z in the form
            [x,y,w1,h1,w2,h2] where x,y is the centre of the box and w (width),h (height)
            """
            w = bbox[4]-bbox[0]
            h = bbox[5]-bbox[1]
            
            x = sum([bbox[0],bbox[2],bbox[4],bbox[6]])/4
            y = sum([bbox[1],bbox[3],bbox[5],bbox[7]])/4
            return np.array([x,y,w,h]).reshape((4,1))

        def convert_kfx_to_bbox(x):
            """
            Takes a bounding box in the centre form [x,y,w,h] and returns it in the form
            [x1,y1,x2,y2,x3,y3,x4,y4]
            """
            w=x[2]
            h=x[3]
            
            p1x = x[0] - w/2
            p1y = x[1] - h/2
            
            p2x = x[0] - w/2
            p2y = x[1] + h/2
            
            p3x = x[0] + w/2
            p3y = x[1] + h/2
            
            p4x = x[0] + w/2
            p4y = x[1] - h/2
            
            return np.array([p1x,p1y,p2x,p2y,p3x,p3y,p4x,p4y]).reshape((1,8))


        class KalmanBoxTracker(object):
            """
            This class represents the internel state of individual tracked objects observed as bbox.
            """
            count = 0
            def __init__(self,bbox):
                """
                Initialises a tracker using initial bounding box.
                """
                from filterpy.kalman import KalmanFilter
                #define constant velocity model
                self.kf = KalmanFilter(dim_x=8, dim_z=4)
                self.kf.F = np.array([
                [1,0,0,0,1,0,0.5,0],
                [0,1,0,0,0,1,0,0.5],
                [0,0,1,0,0,0,0,0],
                [0,0,0,1,0,0,0,0],
                [0,0,0,0,1,0,1,0],
                [0,0,0,0,0,1,0,1],
                [0,0,0,0,0,0,1,0],
                [0,0,0,0,0,0,0,1]])
                self.kf.H = np.array([
                [1,0,0,0,0,0,0,0],
                [0,1,0,0,0,0,0,0],
                [0,0,1,0,0,0,0,0],
                [0,0,0,1,0,0,0,0]])
                
                self.kf.R[:,:] *= 25# set measurement uncertainty for positions
                self.kf.Q[:2,:2] = 0.0 # process uncertainty for positions is zero - only moves due to velocity, leave process for width height as 1 to account for turning
                self.kf.Q[2:4,2:4] *= 0.1# process uncertainty for width/height for turning
                self.kf.Q[4:6,4:6] = 0.0# process uncertainty for velocities is zeros - only accelerates due to accelerations
                self.kf.Q[6:,6:] *= 0.01# process uncertainty for acceleration
                self.kf.P[4:,4:] *= 5# maximum speed

                z=yolo.convert_bbox_to_kfx(bbox)
                self.kf.x[:4] = z
                self.time_since_update = 0
                self.id = yolo.KalmanBoxTracker.count
                yolo.KalmanBoxTracker.count += 1
                self.hits = 1
                self.hit_streak = 1
                self.age = 1
                self.score = bbox[8]

            def update(self,bbox):
                """
                Updates the state vector with observed bbox.
                """
                # Scale the velocity components
                self.time_since_update = 0
                self.hits += 1
                self.hit_streak += 1
                self.score = (self.score*(self.hits-1.0)/float(self.hits)) + (bbox[8]/float(self.hits))
                z = yolo.convert_bbox_to_kfx(bbox)
                self.kf.update(z)

            def predict(self):
                """
                Advances the state vector and returns the predicted bounding box estimate.
                """
                self.kf.predict()
                self.age += 1
                if(self.time_since_update>0):
                    self.hit_streak = 0
                self.time_since_update += 1

            def get_state(self):
                """
                Returns the current bounding box estimate.
                """
                return convert_kfx_to_bbox(self.kf.x)

            def get_distance(self, y):
                """
                Returns the mahalanobis distance to the given point.
                """
                b1 = convert_kfx_to_bbox(self.kf.x[:8])[0]
                return (bbox_iou(b1,y))

        def associate_detections_to_trackers(detections,trackers,iou_threshold = 0.3):
            """
            Assigns detections to tracked object (both represented as bounding boxes)
            Returns 3 lists of matches, unmatched_detections and unmatched_trackers
            """
            from scipy.optimize import linear_sum_assignment
            if(len(trackers)==0):
                return np.empty((0,2),dtype=int), np.arange(len(detections)), np.empty((0,9),dtype=int)

            iou_matrix = np.zeros((len(detections),len(trackers)),dtype=np.float32)
            id_matrix = np.zeros((len(detections),len(trackers)),dtype=np.float32)
            scale_id = 0.5
            
            #detections[:,3] = detections[:,1] + detections[:,2] - detections[:,0]#modified this code here
            for d,det in enumerate(detections):
                for t,trk in enumerate(trackers):
                    trackBox = yolo.convert_kfx_to_bbox(trk.kf.x[:8])[0]#This is the predicted box
                    iou_matrix[d,t] = yolo.bbox_iou(trackBox, det[:-1])#Prop. overlap between boxes
                    id_matrix[d,t] = scale_id*det[8]#prob that det[1] is tracker[t]; tracker is prediction(from previous step), det is present detection

            matched_indices = linear_sum_assignment(-iou_matrix-id_matrix) #minimize a combined cost matrix; IOU weighted by ID
            matched_indices = np.asarray(matched_indices)
            matched_indices = np.transpose(matched_indices)

            unmatched_detections = []
            for d,det in enumerate(detections):
                if(d not in matched_indices[:,0]):
                    unmatched_detections.append(d)

            unmatched_trackers = []
            for t,trk in enumerate(trackers):
                if(t not in matched_indices[:,1]):
                    unmatched_trackers.append(t)

            #filter out matched with low probability
            matches = []
            for m in matched_indices:
                if(iou_matrix[m[0],m[1]]<iou_threshold):
                    unmatched_detections.append(m[0])
                    unmatched_trackers.append(m[1])
                else:
                    matches.append(m.reshape(1,2))

            if(len(matches)==0):
                matches = np.empty((0,2),dtype=int)
            else:
                matches = np.concatenate(matches,axis=0)

            return matches, np.array(unmatched_detections), np.array(unmatched_trackers)



        class yoloTracker(object):
            def __init__(self,max_age=1,track_threshold=0.5, init_threshold=0.9, init_nms=0.5,link_iou=0.05):
                """
                Sets key parameters for YOLOtrack
                """
                self.max_age = max_age # time since last detection to delete track
                self.trackers = []
                self.frame_count = 0
                self.track_threshold = track_threshold # only return tracks with average confidence above this value
                self.init_threshold = init_threshold # threshold confidence to initialise a track, note this is much higher than the detection threshold
                self.init_nms = 0.5#init_nms; threshold overlap to initialise a track - set to 0 to only initialise if not overlapping another tracked detection
                self.link_iou = link_iou # only link tracks if the predicted box overlaps detection by this amount
                yolo.KalmanBoxTracker.count = 0

            def update(self,dets,trk_dets,frame_pos):
                """
                Params:
                  dets - a numpy array of detections in the format [[x1,y1,x2,y2,score],[x1,y1,x2,y2,score],...]
                Requires: this method must be called once for each frame even with empty detections.
                Returns a similar array, where the last column is the object ID.

                NOTE: The number of objects returned may differ from the number of detections provided.
                """
                self.frame_count += 1
                #get predicted locations from existing trackers.
                trks = np.zeros((len(self.trackers),9))

                ret = []
                for t,trk in enumerate(self.trackers):
                    self.trackers[t].predict()

                matched, unmatched_dets, unmatched_trks = yolo.associate_detections_to_trackers(dets,self.trackers, self.link_iou)#self.link_iou
                
                #update matched trackers with assigned detections
                for t,trk in enumerate(self.trackers):
                    if(t not in unmatched_trks):
                        d = matched[np.where(matched[:,1]==t)[0],0] #dets row id
                        trk.update(dets[d,:][0])
                        trk_dets = np.row_stack((trk_dets, np.concatenate((dets[d,0:9].flatten(),np.array([trk.id]),np.array([1]),np.array([0]),np.array([frame_pos])))))#appending matched dets; pix-coords, confidence, ID, tracking or not, time since update, frame number
                        dets[d,8]=2.0 # once assigned we set it to full certainty

                #add tracks to detection list
                for t,trk in enumerate(self.trackers):
                    if(t in unmatched_trks):

                        d = yolo.convert_kfx_to_bbox(trk.kf.x)[0]
                        d = np.append(d,np.array([2]), axis=0)
                        d = np.expand_dims(d,0)
                        dets = np.append(dets,d, axis=0)
                        last_pos = np.where(trk_dets[:,9]==t)[0][-1]
                        trk_dets[last_pos,10]=0#whether tracking on frame
                        trk_dets[last_pos,11] += 1#time since update
                        
                if len(dets)>0:
                    dets = dets[dets[:,8]>self.init_threshold]
                    dets = yolo.do_nms(dets,self.init_nms)
                    dets_copy = dets
                    #For unmatched_dets
                    sort_index = np.where((dets[:,8]<1.1) & (dets[:,8]>0))
                    dets= dets[dets[:,8]<1.1]
                    dets= dets[dets[:,8]>0]
                    

                for d,det in enumerate(dets):
                    trk = yolo.KalmanBoxTracker(det[:])
                    self.trackers.append(trk)
                    trk_dets = np.row_stack((trk_dets, np.concatenate((dets_copy[sort_index[0][d],0:9].flatten(),np.array([trk.id]),np.array([1]),np.array([0]),np.array([frame_pos])))))

                i = len(self.trackers)
                for trk in reversed(self.trackers):
                    i -= 1
                    #remove dead tracklet
                    if(trk.time_since_update > self.max_age):
                        self.trackers.pop(i)

                for trk in (self.trackers):
                    d = yolo.convert_kfx_to_bbox(trk.kf.x)[0]
                    if ((trk.time_since_update < 1) and (trk.score>self.track_threshold)):
                        ret.append(np.concatenate((d,[trk.id])).reshape(1,-1))
                
                if(len(ret)>0):
                    return np.concatenate(ret), trk_dets
                return np.empty((0,13))
